import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime, timedelta

TICKER_TO_COMPANY = {
    "SBER": ["–°–±–µ—Ä–±–∞–Ω–∫", "–°–±–µ—Ä", "–°–±–µ—Ä–∞", "–°–±–µ—Ä–µ"],
    "GAZP": ["–ì–∞–∑–ø—Ä–æ–º", "–ì–∞–∑–ø—Ä–æ–º–∞"],
    "LKOH": ["–õ—É–∫–æ–π–ª", "–õ—É–∫–æ–∏–ª", "–õ—É–∫–æ–π–ª–∞"],
    "YNDX": ["–Ø–Ω–¥–µ–∫—Å", "Yandex"],
    "ROSN": ["–†–æ—Å–Ω–µ—Ñ—Ç—å", "–†–æ—Å–Ω–µ—Ñ—Ç–∏"],
    "TATN": ["–¢–∞—Ç–Ω–µ—Ñ—Ç—å", "–¢–∞—Ç–Ω–µ—Ñ—Ç–∏"],
    "VTBR": ["–í–¢–ë"],
    "MGNT": ["–ú–∞–≥–Ω–∏—Ç", "–ú–∞–≥–Ω–∏—Ç–∞"],
    "NVTK": ["–ù–æ–≤–∞—Ç—ç–∫", "–ù–æ–≤–∞—Ç—ç–∫–∞"],
    "GMKN": ["–ù–æ—Ä–∏–ª—å—Å–∫–∏–π –Ω–∏–∫–µ–ª—å", "–ù–æ—Ä–Ω–∏–∫–µ–ª—å", "–ì–ú–ö"],
    "CHMF": ["–°–µ–≤–µ—Ä—Å—Ç–∞–ª—å"],
    "ALRS": ["–ê–ª—Ä–æ—Å–∞", "–ê–õ–†–û–°–ê"],
    "POLY": ["–ü–æ–ª—é—Å", "–ü–æ–ª—é—Å–∞"],
    "AFKS": ["–°–∏—Å—Ç–µ–º–∞", "–ê–§–ö –°–∏—Å—Ç–µ–º–∞"],
    "MOEX": ["–ú–æ—Å–±–∏—Ä–∂–∞", "MOEX"],
    "MTSS": ["–ú–¢–°"],
    "PHOR": ["–§–æ—Å–∞–≥—Ä–æ", "–§–æ—Å–ê–≥—Ä–æ"],
    "PLZL": ["–ü–æ–ª–∏–º–µ—Ç–∞–ª–ª", "–ü–æ–ª—é—Å –ó–æ–ª–æ—Ç–æ"],
    "RUAL": ["–†—É—Å–∞–ª", "–†–£–°–ê–õ"],
    "TRNFP": ["–¢—Ä–∞–Ω—Å–Ω–µ—Ñ—Ç—å", "–¢—Ä–∞–Ω—Å–Ω–µ—Ñ—Ç–∏"],
    "AKRN": ["–ê–∫—Ä–æ–Ω"],
    "AFLT": ["–ê—ç—Ä–æ—Ñ–ª–æ—Ç", "Aeroflot"],
    "IRAO": ["–ò–Ω—Ç–µ—Ä –†–ê–û", "–ò–Ω—Ç–µ—Ä–†–ê–û"],
}

class APIClient:
    def __init__(self):
        self.headers = {"User-Agent": "Mozilla/5.0"}

    @staticmethod
    def parse_interfax_section(url: str) -> str:
        try:
            response = requests.get(url, timeout=10)
            response.encoding = "windows-1251"
            soup = BeautifulSoup(response.text, "html.parser")

            # meta-—Ç–µ–≥ —Å –∫–∞—Ç–µ–≥–æ—Ä–∏–µ–π
            meta_tag = soup.find("meta", {"property": "article:section"})
            if meta_tag and meta_tag.get("content"):
                return meta_tag["content"]

            # –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞: <aside class="textML"><a>...</a>
            aside = soup.find("aside", class_="textML")
            if aside:
                a_tag = aside.find("a")
                if a_tag and a_tag.text.strip():
                    return a_tag.text.strip()
        except Exception as e:
            print(f"[parser] –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–∏ —Ä–∞–∑–¥–µ–ª–∞: {e}")
        return "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"

    def fetch_stock_data(self, ticker: str, start_date: str, end_date: str) -> pd.DataFrame:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –æ —Ü–µ–Ω–∞—Ö –∞–∫—Ü–∏–π —Å MOEX.
        """
        url = f"https://iss.moex.com/iss/history/engines/stock/markets/shares/securities/{ticker}.json"
        params = {'from': start_date, 'till': end_date, 'start': 0}
        all_records = []

        try:
            while True:
                response = requests.get(url, params=params)
                if response.status_code != 200:
                    print(f"–û—à–∏–±–∫–∞ API {response.status_code}: {response.text}")
                    break

                data = response.json()
                if 'history' in data and 'data' in data['history']:
                    records = data['history']['data']
                    columns = data['history']['columns']
                    if not records:
                        break
                    all_records.extend(records)
                    if len(records) < 100:
                        break
                    params['start'] += 100
                else:
                    print("–û—Ç–≤–µ—Ç API –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –¥–∞–Ω–Ω—ã—Ö.")
                    break

            if not all_records:
                print(f"–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –ø–æ {ticker} –∑–∞ —É–∫–∞–∑–∞–Ω–Ω—ã–π –ø–µ—Ä–∏–æ–¥.")
                return pd.DataFrame()

            df = pd.DataFrame(all_records, columns=columns)
            df["TRADEDATE"] = pd.to_datetime(df["TRADEDATE"], errors="coerce")
            df["CLOSE"] = pd.to_numeric(df["CLOSE"], errors="coerce")
            df = df.dropna(subset=["TRADEDATE", "CLOSE"])
            print(f"–î–∞–Ω–Ω—ã–µ –¥–ª—è {ticker} —É—Å–ø–µ—à–Ω–æ –ø–æ–ª—É—á–µ–Ω—ã. –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {len(df)}")
            return df[["TRADEDATE", "CLOSE"]]
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö: {e}")
            return pd.DataFrame()

    def detect_significant_changes(self, df: pd.DataFrame, threshold: float = 5.0) -> pd.DataFrame:
        """
        –ù–∞—Ö–æ–¥–∏—Ç –¥–Ω–∏ —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è–º–∏ —Ü–µ–Ω –∞–∫—Ü–∏–π.
        """
        df = df.copy()
        df["TRADEDATE"] = pd.to_datetime(df["TRADEDATE"])
        df = df.groupby("TRADEDATE").agg({"CLOSE": "mean"}).reset_index()
        df['DAILY_CHANGE'] = df['CLOSE'].pct_change() * 100
        df = df.dropna()
        significant = df[abs(df['DAILY_CHANGE']) > threshold]
        print(f"–ù–∞–π–¥–µ–Ω–æ {len(significant)} –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π.")
        return significant[["TRADEDATE", "DAILY_CHANGE"]]

    def fetch_news_from_interfax(self, ticker: str, target_date: str) -> pd.DataFrame:
        """
        –ü–∞—Ä—Å–∏—Ç –≤—Å–µ –Ω–æ–≤–æ—Å—Ç–∏ –∑–∞ target_date –∏ –¥–≤–∞ –¥–Ω—è –¥–æ –Ω–µ–≥–æ.
        """
        base_url = "https://www.interfax.ru/news/"
        date_obj = datetime.strptime(target_date, "%Y-%m-%d")
        date_range = [(date_obj - timedelta(days=i)).strftime("%Y/%m/%d") for i in range(2, -1, -1)]
        keywords = TICKER_TO_COMPANY.get(ticker, [])
        all_news = []

        for date in date_range:
            url = f"{base_url}{date}"
            try:
                response = requests.get(url, headers=self.headers)
                if response.status_code != 200:
                    print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {url}: {response.status_code}")
                    continue

                soup = BeautifulSoup(response.text, "html.parser")
                articles = soup.find_all("div", attrs={"data-id": True})

                for article in articles:
                    try:
                        time_tag = article.find("span")
                        title_tag = article.find("h3")
                        link_tag = article.find("a")
                        if not (time_tag and title_tag and link_tag):
                            continue

                        news_time = f"{date} {time_tag.text.strip()}"
                        title = title_tag.text.strip()
                        url = "https://www.interfax.ru" + link_tag["href"]
                        section = link_tag["href"].strip("/").split("/")[0]

                        if any(kw.lower() in title.lower() for kw in keywords):
                            all_news.append({
                                "date": news_time,
                                "title": title,
                                "url": url,
                                "section": section
                            })
                    except Exception as e:
                        print(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}")
                        continue
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {date}: {e}")
                continue

        df = pd.DataFrame(all_news)
        print(f"–ù–∞–π–¥–µ–Ω–æ {len(df)} –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ {ticker} –∑–∞ {date_range}")
        return df

    def fetch_news_for_significant_days(self, ticker: str, df: pd.DataFrame, threshold: float = 5.0) -> pd.DataFrame:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–Ω–µ–π —Å —Å–∏–ª—å–Ω—ã–º–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è–º–∏.
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã –Ω–æ–≤–æ—Å—Ç–µ–π, —á—Ç–æ–±—ã –¥–æ—Å—Ç–∞—Ç—å —Ç–æ—á–Ω—ã–π —Ä–∞–∑–¥–µ–ª (section).
        """
        significant = self.detect_significant_changes(df, threshold)
        all_news = pd.DataFrame()

        for _, row in significant.iterrows():
            target_date = row["TRADEDATE"].strftime("%Y-%m-%d")
            print(f"\n–ü–∞—Ä—Å–∏–Ω–≥ –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è {ticker} –Ω–∞ –¥–∞—Ç—É {target_date}")
            news = self.fetch_news_from_interfax(ticker, target_date)

            if not news.empty:
                all_news = pd.concat([all_news, news], ignore_index=True)

        print(f"\n–ò—Ç–æ–≥: –ù–∞–π–¥–µ–Ω–æ {len(all_news)} –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è {len(significant)} –∑–Ω–∞—á–∏–º—ã—Ö –¥–Ω–µ–π.")
        return all_news

    def fetch_news_from_interfax_range(self, ticker: str, start_date: datetime.date, end_date: datetime.date) -> pd.DataFrame:
        """
        –ü–æ–ª—É—á–∞–µ—Ç –≤—Å–µ –Ω–æ–≤–æ—Å—Ç–∏ –∑–∞ –¥–∏–∞–ø–∞–∑–æ–Ω –±–µ–∑ ¬±2 –¥–Ω—è.
        """
        all_news = []
        keywords = TICKER_TO_COMPANY.get(ticker, [])

        for offset in range((end_date - start_date).days + 1):
            date_str = (start_date + timedelta(days=offset)).strftime("%Y/%m/%d")
            url = f"https://www.interfax.ru/business/news/{date_str}"
            try:
                response = requests.get(url, headers=self.headers)
                if response.status_code != 200:
                    continue

                soup = BeautifulSoup(response.text, 'html.parser')
                articles = soup.find_all("div", attrs={"data-id": True})

                for article in articles:
                    try:
                        time_tag = article.find("span")
                        title_tag = article.find("h3")
                        link_tag = article.find("a")
                        if not (time_tag and title_tag and link_tag):
                            continue

                        title = title_tag.text.strip()
                        url = "https://www.interfax.ru" + link_tag["href"]
                        section = link_tag["href"].strip("/").split("/")[0]
                        news_time = f"{date_str.replace('/', '-')} {time_tag.text.strip()}"

                        if any(kw.lower() in title.lower() for kw in keywords):
                            all_news.append({
                                "date": news_time,
                                "title": title,
                                "url": url,
                                "section": section
                            })
                    except:
                        continue
            except:
                continue

        return pd.DataFrame(all_news)

# ===== –¢–µ—Å—Ç–æ–≤—ã–π main =====
if __name__ == "__main__":
    client = APIClient()
    ticker = "SBER"
    test_date = (datetime.today() - timedelta(days=2)).strftime("%Y-%m-%d")

    print(f"\n–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ interfax.ru –¥–ª—è {test_date} ({ticker})")
    news = client.fetch_news_from_interfax(ticker, test_date)

    if news.empty:
        print("‚ö† –ù–æ–≤–æ—Å—Ç–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –∏–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è.")
    else:
        print(f"–ù–∞–π–¥–µ–Ω–æ {len(news)} –Ω–æ–≤–æ—Å—Ç–µ–π. –ü—Ä–∏–º–µ—Ä—ã:")
        for i, row in news.head(5).iterrows():
            print(f"\nüóì {row['date']}")
            print(f"–ó–∞–≥–æ–ª–æ–≤–æ–∫: {row['title']}")
            print(f"–†–∞–∑–¥–µ–ª: {row.get('section', '–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}")
            print(f"–°—Å—ã–ª–∫–∞: {row['url']}")
